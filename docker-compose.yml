version: "3.8"

x-airflow-common: &airflow-common
  build: ./airflow      # <-- use the custom Dockerfile
  env_file:
    - .env
  environment:
    # Core Airflow settings
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__FERNET_KEY: "please_generate_a_real_fernet_key"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
    AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW__CORE__DEFAULT_TIMEZONE:-UTC}
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "true"

    # Email backend for notifications
    AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"

    # SMTP settings wired from .env
    AIRFLOW__SMTP__SMTP_HOST: ${SMTP_HOST}
    AIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT}
    AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER}
    AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD}
    AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_FROM}
    AIRFLOW__SMTP__SMTP_STARTTLS: "true"
    AIRFLOW__SMTP__SMTP_SSL: "false"

    # Allow running as non-root user inside container
    AIRFLOW_UID: ${AIRFLOW_UID}
    AIRFLOW_GID: ${AIRFLOW_GID}

  volumes:
    # Airflow project code
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/include:/opt/airflow/include

    # Shared data volume between Airflow and other services
    - ./shared_data:/shared_data

    # Persist processed file tracking JSON
    - ./airflow/processed_drive_files.json:/opt/airflow/processed_drive_files.json

  user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
  depends_on:
    - postgres

services:
  # ======================
  # Postgres (Airflow DB)
  # ======================
  postgres:
    image: postgres:15
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  # ======================
  # MinIO (S3-compatible)
  # ======================
  minio:
    image: minio/minio:latest
    restart: always
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    ports:
      - "9000:9000"  # S3 API endpoint
      - "9001:9001"  # MinIO console UI

  # ======================
  # Airflow init
  # ======================
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Initializing Airflow database..."
        airflow db migrate

        echo "Creating Airflow admin user (if not exists)..."
        airflow users create \
          --role Admin \
          --username "${AIRFLOW_ADMIN_USERNAME}" \
          --password "${AIRFLOW_ADMIN_PASSWORD}" \
          --firstname "${AIRFLOW_ADMIN_FIRSTNAME}" \
          --lastname "${AIRFLOW_ADMIN_LASTNAME}" \
          --email "${AIRFLOW_ADMIN_EMAIL}" || true

        echo "Airflow init completed."
    restart: "no"
    depends_on:
      - postgres

  # ======================
  # Airflow webserver
  # ======================
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    restart: always
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - airflow-init
      - minio

  # ======================
  # Airflow scheduler
  # ======================
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-init
      - minio

volumes:
  postgres-db-volume:
  minio-data:
